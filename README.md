# Sparkify
Sparkify is a music startup that wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. The data resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app. They want to create a Postgres database with tables designed to optimize song playback analysis queries. So, the purpose of this project is to create a database schema and ETL pipeline for this analysis
# Datasets
Song Dataset - subset of the [Million Song Dataset](http://millionsongdataset.com) (JSON format) and contains metadata about the song and artist Log Dataset - simulated activity logs created based on the song dataset.

Log Dataset - consists of log files in JSON format generated by [this event](https://github.com/Interana/eventsim) simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.
# Database schema
The database schema will be a star schema and will contain a *fact table (songplays)* and *four dimension tables (songs, artists, time, users)*

*songplays* - data related to songplays

*users* - user data from Sparkify application(Log Dataset)  
*songs* - songs data from the Song Dataset  
*artists* - artists data from the Song Dataset  
*time* - timestamps which are broken into smaller units from the Log Dataset 

![](https://c.radikal.ru/c12/2012/72/27a1933edde3.jpg)

# Python scripts
## sql_queries.py
Contains all the CREATE statements, INSERT statements, and DROP statements.

## Create_tables.py
Creates and drops all the tables.

## test.ipynb
Tests the database by querying few rows of the database.

## etl.ipynb
Reads and processes the song and log data and loads them to the tables.

## etl.py
Performs the same functions as etl.ipynb but is the final script that runs an ETL process that is persisted to the database.

# ETL processes:

## Song file:

- Convert the song file into a dataframe(df)
- Insert the subset of df into the song table
- Insert the subset of df into the artist table

## Processing the log file:

- Convert the log file into a dataframe(df)
- Filter the dataframe by the "NextSong" action(column:page)
- Convert timestamp column into a datetime format
- Insert the subset of df into the user table
- Get song and artist column from the song and artist table and insert the rows into the songplay table

# How to run the Python scripts
All python scripts can be run on the terminal. The steps to run the tables are as follows:
- Run **create_tables.py** to create the tables
- Run **etl.py** will contain the functions that will read/process the JSON files from their respective directories and insert the data into the appropriate tables.
- Use the **test.ipynb** notebook to see if the data has been inserted properly
